
{% load_yaml as extended_defaults %}

/etc/lxc/lxc.conf
lxc.lxcpath
    # The location in which all containers are stored.
lxc.default_config
    # The path to the default container configuration.
lxc.cgroup.use
    # Comma separated list of cgroup controllers to setup. If none is specified, all available controllers will be used.
lxc.cgroup.pattern
    # Format string used to generate the cgroup path (e.g. lxc/%n).
lxc.bdev.lvm.vg
    # Default LVM volume group name.
lxc.bdev.lvm.thin_pool
    # Default LVM thin pool name.
lxc.bdev.zfs.root
    # Default ZFS root name.

gateway: the ipv4 gateway to use the default does nothing more than lxcutils does
bridge the bridge to use the default does nothing more than lxcutils does
network_profile {# Network profile to use for the container #}
nic_opts: {# Extra options for network interfaces, will override #}
users:   Users for which the password defined in the password param should be set. Can be passed as a comma separated list or a python list. Defaults to just the root user.
password:  Set the initial password for the users defined in the users parameter
password_encrypted False
    Set to True to denote a password hash instead of a plaintext password
profile  A LXC profile (defined in config or pillar). This can be either a real profile mapping or a string to retrieve it in configuration
start   Start the newly-created container
dnsservers  list of dns servers to set in the container, default [] (no setting)

install If salt-minion is not already installed, install it. Default: True
seed    Seed the container with the minion config. Default: True
config Optional config parameters. By default, the id is set to the name of the container.
master salt master (default to minion's master)
master_port  salt master port (default to minion's master port)
pub_key  Explicit public key to preseed the minion with (optional). This can be either a filepath or a string representing the key
priv_key  Explicit private key to preseed the minion with (optional). This can be either a filepath or a string representing the key
approve_key  If explicit preseeding is not used; Attempt to request key approval from the master. Default: True
path  path to the container parent directory default: /var/lib/lxc (system)

clone_from    Original from which to use a clone operation to create the container. Default: None
bootstrap_delay    Delay in seconds between end of container creation and bootstrapping. Useful when waiting for container to obtain a DHCP lease.
bootstrap_url    See lxc.bootstrap
bootstrap_shell    See lxc.bootstrap
bootstrap_args    See lxc.bootstrap
force_install    Force installation even if salt-minion is detected, this is the way to run vendor bootstrap scripts even if a salt minion is already present in the container
unconditional_install  Run the script even if the container seems seeded

    - name: gui
      config:
        environment.DISPLAY: :0
        raw.idmap: both 1000 1000
        user.user-data: |
          #cloud-config
          runcmd:
            - 'sed -i "s/; enable-shm = yes/enable-shm = no/g" /etc/pulse/client.conf'
            - 'echo export PULSE_SERVER=unix:/tmp/.pulse-native | tee --append /home/ubuntu/.profile'
          packages:
            - x11-apps
            - mesa-utils
            - pulseaudio
      description: LXD Gui Profile
      devices:
        PASocket:
          path: /tmp/.pulse-native
          source: /run/user/1000/pulse/native
          type: disk
        X0:
          path: /tmp/.X11-unix/X0
          source: /tmp/.X11-unix/X0
          type: disk
        mygpu:
          type: gpu

# Storage pools
storage_pools:
- name: {{ settings.defaults.storage_name }}
  description: LXD {{ settings.defaults.storage_name }} pool
  driver: dir
  config:
    source: /var/lib/lxd/storage-pools/{{ settings.defaults.storage_name }}

# self managed Network devices
networks:
- name: {{ settings.defaults.bridge_name }}
  type: bridge
  config:
    ipv4.address: {{ settings.defaults.bridge_cidr }}
    ipv4.nat: true
    ipv6.address: none
    # needed parameter to prevent dns loop and point host dns to lxd for *.lxd
    raw.dnsmasq: |
        auth-zone=lxd
        dns-loop-detect

# Profiles
profiles:
- name: default
  description: Default LXD profile
    devices:
    eth0:
      nictype: bridged
      parent: {{ settings.defaults.bridge_name }}
      type: nic
    root:
      path: /
      pool: {{ settings.defaults.storage_name }}
      type: disk
- name: autostart
  config:
    boot.autostart: true
    boot.autostart.delay: 2
- name: nested
  config:
    security.nesting: true
- name: network_extra
  config:
    linux.kernel_modules: ip_tables,ip6_tables,netlink_diag,nf_nat,xt_conntrack,br_netfilter,nf_conntrack,ip_vs,ip_vs_rr,ip_vs_wrr,ip_vs_sh
- name: syscalls_intercept
  config:
    security.syscalls.intercept.mknod: true
    security.syscalls.intercept.setxattr: true

# Images
images:
- name: bionic
  public: true
  auto_update: true
  source:
    name: bionic/amd64
    remote: ubuntu
- name: focal
  public: true
  auto_update: true
  source:
    name: focal/amd64
    remote: ubuntu
- name: focal-daily
  public: true
  auto_update: true
  source:
    name: focal/amd64
    remote: ubuntu-daily
{% endload %}


{%- for section in ['storage_pools', 'networks', 'profiles', 'images'] %}
  {%- set section_org = salt['pillar.get']('lxd:'+ section, []) %}
  {%- set section_names= section_org|map(attribute='name') %}
  {%- for item in extended_defaults[section] %}
    {%- if item.name not in section_names %}
      {%- do section_org.append(item) %}
    {%- endif %}
  {%- endfor %}
  {%- do settings.update({section: section_org}) %}
{%- endfor %}

lxc.apparmor.allow_incomplete
lxc.apparmor.allow_nesting
lxc.apparmor.profile
lxc.apparmor.profile = generated
lxc.apparmor.profile = unchanged
lxc.apparmor.profile = unconfined
lxc.apparmor.raw
lxc.arch
lxc.autodev
lxc.autodev.tmpfs.size
lxc.cap.drop
lxc.cap.drop = mac_override
lxc.cap.drop = sys_module mknod setuid net_raw
lxc.cap.keep
lxc.cgroup2.[controller name]
lxc.cgroup.[controller name]
lxc.cgroup.cpuset.cpus = 0,1
lxc.cgroup.cpu.shares = 1234
lxc.cgroup.devices.allow = b 8:0 rw
lxc.cgroup.devices.allow = c 1:3 rw
lxc.cgroup.devices.deny = a
lxc.cgroup.dir
lxc.cgroup.dir  =  my-cgroup/first for a container named "c1" will create the container's cgroup as a
lxc.cgroup.relative
lxc.console.buffer.size
lxc.console.buffer.size and lxc.console.buffer.logfile.
lxc.console.logfile
lxc.console.path
lxc.console.rotate
lxc.console.size
lxc.container.conf(5)                                                                         lxc.container.conf(5)
lxc.container.conf - LXC container configuration file
lxc.environment
lxc.environment = APP_ENV=production
lxc.environment = PATH
lxc.environment = SYSLOG_SERVER=192.0.2.42
lxc.ephemeral
lxc.execute.cmd
lxc.group
lxc.hook.autodev
lxc.hook.clone
lxc.hook.destroy
lxc.hook.mount
lxc.hook.post-stop
lxc.hook.pre-mount
lxc.hook.pre-start
lxc.hook.start
lxc.hook.start-host
lxc.hook.stop
lxc.hook.version
lxc.hook.version config item. If it is set to 0 then old-style hooks are used. If it is set to 1 then new-
lxc.idmap
lxc.idmap = g 0 100000 10000
lxc.idmap = u 0 100000 10000
lxc.include
lxc.init.cmd
lxc.init.cwd
lxc.init.gid
lxc.init.uid
lxc.keyring.session
lxc.keyring.session = 0
lxc.log.file
lxc.log.level
lxc.log.syslog
lxc.monitor.signal.pdeath
lxc.monitor.unshare
lxc.mount.auto
lxc.mount.auto = proc:rw sys:rw cgroup-full:rw
lxc.mount.auto = proc sys cgroup
lxc.mount.entry
lxc.mount.entry = /lib /root/myrootfs/lib none ro,bind 0 0
lxc.mount.fstab
lxc.mount.fstab = /etc/fstab.complex
lxc.namespace.clone
lxc.namespace.keep
lxc.namespace.share.[namespace identifier]
lxc.namespace.share.user=/opt/c3.
lxc.net
lxc.net.0.flags = up
lxc.net.0.hwaddr = 4a:49:43:49:79:bf
lxc.net.0.ipv4.address = 10.2.3.5/24 10.2.3.255
lxc.net.0.ipv6.address = 2003:db8:1:0:214:1234:fe0b:3597
lxc.net.0.ipv6.address = 2003:db8:1:0:214:5432:feab:3588
lxc.net.0.link = br0
lxc.net.0.name = eth0
lxc.net.0.type = veth
lxc.net.1.flags = up
lxc.net.1.hwaddr = 4a:49:43:49:79:bd
lxc.net.1.ipv4.address = 10.2.3.4/24
lxc.net.1.ipv4.address = 192.168.10.125/24
lxc.net.1.ipv6.address = 2003:db8:1:0:214:1234:fe0b:3596
lxc.net.1.link = eth0
lxc.net.1.type = macvlan
lxc.net.2.flags = up
lxc.net.2.hwaddr = 4a:49:43:49:79:ff
lxc.net.2.ipv4.address = 10.2.3.6/24
lxc.net.2.ipv6.address = 2003:db8:1:0:214:1234:fe0b:3297
lxc.net.2.link = dummy0
lxc.net.2.type = phys
lxc.net.[i].flags
lxc.net.[i].hwaddr
lxc.net.[i].ipv4.address
lxc.net.[i].ipv4.gateway
lxc.net.[i].ipv6.address
lxc.net.[i].ipv6.gateway
lxc.net.[i].l2proxy
lxc.net.[i].link
lxc.net.[i].mtu
lxc.net.[i].name
lxc.net.[i].script.down
lxc.net.[i].script.up
lxc.net.[i].type
lxc.no_new_privs
lxc.prlimit.[limit name]
lxc.proc.oom_score_adj = 10
lxc.proc.[proc file name]
lxc.pty.max
lxc.rootfs.managed
lxc.rootfs.mount
lxc.rootfs.options
lxc.rootfs.path
lxc.rootfs.path = dir:/mnt/rootfs.complex
lxc.seccomp.allow_nesting
lxc.seccomp.notify.cookie
lxc.seccomp.notify.proxy
lxc.seccomp.profile
lxc.selinux.context
lxc.selinux.context.keyring
lxc.selinux.context.keyring = system_u:system_r:lxc_t:s0:c22
lxc.selinux.context = system_u:system_r:lxc_t:s0:c22
lxc.signal.halt
lxc.signal.reboot
lxc.signal.stop
lxc.start.auto
lxc.start.auto == 1 that is a member of the "onboot" group. The startup will be in order of lxc.start.order.
lxc.start.auto  ==  1  which  are  not  members of any group (the NULL group) and proceed as with the onboot
lxc.start.delay
lxc.start.order
lxc.sysctl.[kernel parameters name]
lxc.tty.dir
lxc.tty.max
lxc.uts.name
lxc.uts.name = complex
lxc.uts.name = myhostname
